{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA_exatraction_with_RegEx.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CabGhWcm13Jq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1\n",
        "\n",
        "* Peyman Naseri, `96100522`\n",
        "* Mobina Pournemat, `97105833`\n",
        "* Mahsa Amani, `97105769`"
      ],
      "metadata": {
        "id": "g5cZzhVqUjQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Drive"
      ],
      "metadata": {
        "id": "kOFJGlUDK6rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S7DhWpNNK8M6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e145446b-3931-42a2-bbee-1f528644d9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "link to folder: https://drive.google.com/drive/folders/1evTdWmxf1lCCKtz7uAQWuQbUoId64nKd?usp=sharing"
      ],
      "metadata": {
        "id": "j5bWViDAZkDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# or where you put the folder\n",
        "%cd /content/drive/MyDrive/NLP/HW1/"
      ],
      "metadata": {
        "id": "fk6KtqFCK77e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e5c686-da95-4d79-ca69-4f75816add4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP/HW1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "c0149SVoS3Vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e194d0-376c-40ea-b527-e94c41feb05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "farsi_names.sql  places.txt\t\t\t  resources\n",
            "parsi_io\t QA_exatraction_with_RegEx.ipynb  verb_to_bon.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "با توجه به این که تعدادی فایل شامل فایل اسامی فارسی، فایل مکان ها، بن فعل ها و نیز tagger موجود در کتابخانه hazm نیاز داریم، آن ها را از فولدر درایو می خوانیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "2ZPb6TlGHAbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import required packages\n"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "CabGhWcm13Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install hazm\n",
        "! pip install parstdex"
      ],
      "metadata": {
        "id": "P6dLb2_dUnyH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b56c167-d252-48f3-adeb-6d7588f16dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 23.5 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 46.8 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=51498f2e911e17cde48b319adb4327ca2278377c330df17ccca08beb65704613\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=155216 sha256=a1142e066e6de06abfc4df43b9672e515df0284a88645a843989dce83927eb1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Collecting parstdex\n",
            "  Downloading parstdex-1.1.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 964 kB/s \n",
            "\u001b[?25hCollecting pytest~=6.2.5\n",
            "  Downloading pytest-6.2.5-py3-none-any.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from parstdex) (1.21.5)\n",
            "Collecting setuptools~=58.0.4\n",
            "  Downloading setuptools-58.0.4-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: hazm~=0.7.0 in /usr/local/lib/python3.7/dist-packages (from parstdex) (0.7.0)\n",
            "Collecting pytextspan==0.5.4\n",
            "  Downloading pytextspan-0.5.4-cp37-cp37m-manylinux2010_x86_64.whl (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 64.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm~=0.7.0->parstdex) (3.3)\n",
            "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from hazm~=0.7.0->parstdex) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm~=0.7.0->parstdex) (1.15.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest~=6.2.5->parstdex) (21.4.0)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest~=6.2.5->parstdex) (1.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest~=6.2.5->parstdex) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest~=6.2.5->parstdex) (4.11.3)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest~=6.2.5->parstdex) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest~=6.2.5->parstdex) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest~=6.2.5->parstdex) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytest~=6.2.5->parstdex) (3.0.8)\n",
            "Installing collected packages: toml, pluggy, setuptools, pytextspan, pytest, parstdex\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed parstdex-1.1.1 pluggy-1.0.0 pytest-6.2.5 pytextspan-0.5.4 setuptools-58.0.4 toml-0.10.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install https://github.com/sobhe/hazm/archive/master.zip --upgrade"
      ],
      "metadata": {
        "id": "gOpN28McF3U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "import json\n",
        "from parstdex import Parstdex\n",
        "from parsi_io.modules.number_extractor import NumberExtractor"
      ],
      "metadata": {
        "id": "-BeqsFj3SU4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "ابتدا تمام نیازمندی ها را نصب و ایمپورت می کنیم.\n",
        "</div>"
      ],
      "metadata": {
        "id": "ZtfdPCowG53k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question & Answer Extraction"
      ],
      "metadata": {
        "id": "Vo7QwiqOUHhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeExtraction(object):\n",
        "    def __init__(self):\n",
        "        self.model = Parstdex()\n",
        "\n",
        "    def run(self, text):\n",
        "        result = self.model.extract_marker(text)\n",
        "        return result"
      ],
      "metadata": {
        "id": "0Jb47GJ22mZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QAExtraction:\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.load_files()\n",
        "        self.tagger = POSTagger(model='resources/postagger.model')\n",
        "        self.live_patterns = self.names + [\"من\", \"تو\", \"او\", \"ما\", \"شما\", \"آن ها\", \"آنها\", \"دوست\", \"رفیق\", \"همسایه\", \"همکار\", \"دشمن\"]\n",
        "\n",
        "    def load_files(self):\n",
        "        # file to find root of a verb\n",
        "        with open('verb_to_bon.json', encoding='utf-8') as f:\n",
        "            self.v_to_thirdperson = json.load(f)\n",
        "\n",
        "        # file containing persian names\n",
        "        with open('farsi_names.sql') as f:\n",
        "            self.names = [eval(line.rstrip('\\n').split(\",\")[1]) for line in f]\n",
        "\n",
        "        # file containing name of places/locations\n",
        "        with open('places.txt') as f:\n",
        "            self.places = f.read().split(\"|\")\n",
        "\n",
        "    def check_live_noun(self, text):\n",
        "        text_arr = text.split()\n",
        "        for t in text_arr:\n",
        "            if t in self.live_patterns:\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "    def check_place(self, text):\n",
        "        if text in self.places:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def transitive_verbs(self, text):\n",
        "        text = text.replace(\"\\u200c\", \" \")\n",
        "        result = []\n",
        "        regexes = re.finditer(r\"\\sرا\\s\", text)\n",
        "        prev_e = 0\n",
        "        for regex in regexes:\n",
        "            s, e = regex.start(), regex.end()\n",
        "            text_arr = text.split()\n",
        "            index_of_ra = text_arr.index(\"را\", prev_e)\n",
        "            prev_e = index_of_ra + 1\n",
        "            A = text_arr[index_of_ra-1]\n",
        "            # check live and not live subject\n",
        "            live_or_not = self.check_live_noun(A)\n",
        "            if live_or_not:\n",
        "                live_vs_not = \" چه کسی را \"\n",
        "            else:\n",
        "                live_vs_not = \" چه چیزی/کسی را \"\n",
        "            Q = \" \".join(text_arr[:index_of_ra-1]) + live_vs_not + text[e:-1] + \"؟\"  \n",
        "            result.append({\"Question\": Q, \"Answer\": A})\n",
        "        return result\n",
        "\n",
        "\n",
        "    def esnadi_verbs(self, text):\n",
        "        text = text.replace(\"\\u200c\", \" \")\n",
        "        result = []\n",
        "        regexes = re.finditer(r\"(است|بود|شد|گشت|گردید|هست|نیست|بود|باشد|باد|شود|می شود|شده است)(م|ی|یم|ید|ند|\\s)\", text)\n",
        "        for regex in regexes:\n",
        "            s, e = regex.start(), regex.end()\n",
        "            Q = \" \".join(text[:e].split()[:-2]) + \" چگونه \" + text[s:e].strip() + \"؟\"\n",
        "            A = text\n",
        "            result.append({\"Question\": Q, \"Answer\": A})\n",
        "        return result\n",
        "\n",
        "\n",
        "    def prepositional_verbs(self, text):\n",
        "        text = text.replace(\"\\u200c\", \" \")\n",
        "        result = []\n",
        "        regexes = re.finditer(r\"\\s(از|به|با|در|برای)\\s\", text)\n",
        "        for regex in regexes:\n",
        "            s, e = regex.start(), regex.end()\n",
        "            # check live and not live subject\n",
        "            A = text[e:].split()[0]\n",
        "            live_or_not = self.check_live_noun(A)\n",
        "            place = self.check_place(A)\n",
        "            if live_or_not:\n",
        "                live_vs_not = \"چه کسی \"\n",
        "            elif place:\n",
        "                live_vs_not = \"کجا \"\n",
        "            else:\n",
        "                live_vs_not = \"چه چیزی \"\n",
        "            Q = text[:e] + live_vs_not + \" \".join(text[e:-1].split()[1:]) + \"؟\"\n",
        "            result.append({\"Question\": Q, \"Answer\": A})\n",
        "        return result\n",
        "\n",
        "\n",
        "    def do_questions(self, text):\n",
        "        result = []\n",
        "        regexes = re.finditer(r\"\\s(زیرا|چون|چرا که|به این دلیل که)\\s\", text)\n",
        "        s = -1\n",
        "        for regex in regexes:\n",
        "            s, e = regex.start(), regex.end()\n",
        "\n",
        "        pos_tags = self.tagger.tag(word_tokenize(text))\n",
        "        text = text.replace(\"\\u200c\", \" \")\n",
        "        for word, tag in pos_tags:\n",
        "            if tag == 'V':\n",
        "                # check pos or neg\n",
        "                if word[0] == \"ن\":\n",
        "                    A = \"خیر.\"\n",
        "                    index_of_n = text.find(word)\n",
        "                    new_text = \"\"\n",
        "                    # remove n\n",
        "                    for i, c in enumerate(text):\n",
        "                        if i != index_of_n:\n",
        "                            new_text += c\n",
        "                    text = new_text\n",
        "                else:\n",
        "                    A = \"بله.\"\n",
        "                # check if there is a describtion after verb\n",
        "                if text.find(word) < s:\n",
        "                    Q = \"آیا \" + text[:s].strip() + \"؟\"\n",
        "                    A += text[s:]\n",
        "                else:\n",
        "                    Q = \"آیا \" + text[:-1] + \"؟\"\n",
        "\n",
        "                result.append({\"Question\": Q, \"Answer\": A})\n",
        "                break\n",
        "        return result\n",
        "\n",
        "\n",
        "    def including_keywords(self, text):\n",
        "        text = text.replace(\"\\u200c\", \" \")\n",
        "        text = text[:-1] if text[-1] is '.' else text\n",
        "        result = []\n",
        "        regexes = re.finditer(r\"\\s(باعث|عامل|دلیل|برهان|موجب)\\s\", text)\n",
        "        for regex in regexes:\n",
        "            s, e = regex.start(), regex.end()\n",
        "            A = text[:s].strip()\n",
        "            live_or_not = self.check_live_noun(A)\n",
        "            if live_or_not:\n",
        "                live_vs_not = \"چه کسی \"\n",
        "            else:\n",
        "                live_vs_not = \"چه چیزی\"\n",
        "            Q = live_vs_not + text[s:] + \"؟\"\n",
        "            result.append({\"Question\": Q, \"Answer\": A})\n",
        "        regexes = re.finditer(r\"\\s(زیرا|چون|چرا که|به این دلیل که)\\s\", text)\n",
        "        for regex in regexes:\n",
        "            s, e = regex.start(), regex.end()\n",
        "            Q = \"چرا \" + text[:s] + \"؟\"\n",
        "            A = \"چون \" + text[e:].strip() + '.'\n",
        "            result.append({\"Question\": Q, \"Answer\": A})\n",
        "        regexes = re.finditer(r\"\\s(می توان نتیجه گرفت)\\s\", text)\n",
        "        for regex in regexes:\n",
        "            s, e = regex.start(), regex.end()\n",
        "            Q = text[:s] + \" چه نتیجه ای می توان گرفت؟\"\n",
        "            A = text[e:].strip() + '.'\n",
        "            result.append({\"Question\": Q, \"Answer\": A})\n",
        "        return result\n",
        "      \n",
        "\n",
        "    def start_with_pronoun(self, text):\n",
        "        text = text[:-1] if text[-1] is '.' else text\n",
        "        pos_tags = self.tagger.tag(word_tokenize(text))\n",
        "        text = text.replace(\"\\u200c\", \" \")\n",
        "        result = []\n",
        "        regexes = re.finditer(r\"^(من|تو|او)\\s\", text)\n",
        "        for regex in regexes:\n",
        "            s, e = regex.start(), regex.end()\n",
        "            for word, tag in pos_tags:\n",
        "                if tag == 'V':\n",
        "                    if self.v_to_thirdperson.get(word) is None:\n",
        "                      return {}\n",
        "                    word = word.replace(\"_\", \" \")\n",
        "                    text = text.replace(word, self.v_to_thirdperson[word])\n",
        "            Q = \"چه کسی \" + text[e:] + \"؟\"\n",
        "            A = text[:e].strip()\n",
        "            result.append({\"Question\": Q, \"Answer\": A})\n",
        "        regexes = re.finditer(r\"^(ما|شما|ایشان|آن ها|آنها)\\s\", text)\n",
        "        for regex in regexes:\n",
        "            s, e = regex.start(), regex.end()\n",
        "            for word, tag in pos_tags:\n",
        "                if tag == 'V':\n",
        "                    if self.v_to_thirdperson.get(word) is None:\n",
        "                      return {}\n",
        "                    word = word.replace(\"_\", \" \")\n",
        "                    text = text.replace(word, self.v_to_thirdperson[word])\n",
        "            Q = \"چه کسانی \" + text[e:] + \"؟\"\n",
        "            A = text[:e]\n",
        "            result.append({\"Question\": Q, \"Answer\": A})\n",
        "        return result\n",
        "    \n",
        "    \n",
        "    def when_questions(self, text):\n",
        "        result = []\n",
        "        if bool(re.search('(زیرا|چون|چرا که|به این دلیل که)', text)):\n",
        "          return result\n",
        "        text = text.replace(\"\\u200c\", \" \")\n",
        "        text = text[:-1] if text[-1] is '.' else text\n",
        "        extractor = TimeExtraction()\n",
        "        extractions = extractor.run(text)\n",
        "        for t in (extractions['datetime'].keys()):\n",
        "            s, e = t[1:-1].split(', ')\n",
        "            s, e = int(s), int(e)\n",
        "            Q = text[:s] + 'چه زمانی' + text[e:] + '؟'\n",
        "            A = text[s:e]\n",
        "            result.append({\"Question\": Q, \"Answer\": A})\n",
        "        return result\n",
        "\n",
        "\n",
        "    def number_question(self,text):\n",
        "        text = text.replace(\"\\u200c\", \" \")\n",
        "        result = []\n",
        "        text = text[:-1] if text[-1] is '.' else text \n",
        "        normalizer = Normalizer()\n",
        "        text = normalizer.normalize(text)\n",
        "        extractor = NumberExtractor()\n",
        "        numbers = extractor.run(text)\n",
        "        if numbers:\n",
        "            for num in numbers:\n",
        "                Q = text[:num['span'][0]] + 'چند' + text[num['span'][1]:] + '؟'\n",
        "                A = num['phrase']\n",
        "                result.append({\"Question\": Q, \"Answer\": A})\n",
        "        return result\n",
        "    \n",
        "    def adjective_additive(self, text):\n",
        "        # text = text[:-1] if text[-1] is '.' else text\n",
        "        normalizer = Normalizer()\n",
        "        text = normalizer.normalize(text)\n",
        "        def check_live(word):\n",
        "            stemmer = Stemmer()\n",
        "            for pat in self.live_patterns:\n",
        "                if stemmer.stem(word) in pat:\n",
        "                    return True\n",
        "            return False \n",
        "\n",
        "        result = []\n",
        "        if bool(re.search('(زیرا|چون|چرا که|به این دلیل که)', text)):\n",
        "            return result\n",
        "        \n",
        "        tagger = POSTagger(model='resources/postagger.model')\n",
        "        word_taggs = tagger.tag(word_tokenize(text))\n",
        "        words = [w[0] for w in word_taggs]\n",
        "        taggs = [w[1] for w in word_taggs]\n",
        "\n",
        "        for i in range(1,len(taggs)):\n",
        "            if taggs[i-1]=='Ne':\n",
        "                if taggs[i]=='N' and check_live(words[i]):\n",
        "                    Q = text.replace(words[i],'چه کسی')\n",
        "                    Q = Q[:-1] if Q[-1] is '.' else Q\n",
        "                    Q = Q + '؟'\n",
        "                    A = words[i-1] + ' ' + words[i]\n",
        "                    result.append({\"Question\": Q, \"Answer\": A})\n",
        "                elif taggs[i] == 'AJ' or (taggs[i]=='N' and not(check_live(words[i]))):\n",
        "                    l = words[:i-1]+ ['چه', words[i-1]+' ی']+words[i+1:]\n",
        "                    Q = \" \".join(l)\n",
        "                    Q = Q[:-1] if Q[-1] is '.' else Q\n",
        "                    Q = Q.strip() + '؟'\n",
        "                    A = words[i-1] + ' ' + words[i]\n",
        "                    result.append({\"Question\": Q, \"Answer\": A})\n",
        "        return result\n",
        "\n",
        "\n",
        "    def run(self, text):\n",
        "        result = []\n",
        "        try:\n",
        "            # check regexes\n",
        "            # transitive verbs \n",
        "            result += self.transitive_verbs(text)\n",
        "            # Esnadi verbs\n",
        "            result += self.esnadi_verbs(text)\n",
        "            # verbs which have prepositions\n",
        "            result += self.prepositional_verbs(text)\n",
        "            # Do, Does Questions\n",
        "            result += self.do_questions(text)\n",
        "            # Keywords\n",
        "            result += self.including_keywords(text)\n",
        "            # starting with pronoun\n",
        "            result += self.start_with_pronoun(text)\n",
        "            # when questions\n",
        "            result += self.when_questions(text)\n",
        "            # questions with numbers\n",
        "            result += self.number_question(text)\n",
        "            # adjective additive\n",
        "            result += self.adjective_additive(text)\n",
        "            return result\n",
        "        except:\n",
        "            return result"
      ],
      "metadata": {
        "id": "CTNuAJ63TG1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=rtl>\n",
        "\n",
        "* **تابع transitive_verbs:** این تابع برای سوال کردن مفعول جمله بکار می رود، بدین صورت که چون \"را\" علامت مفعولی است، با استفاده از یافتن  آن می توان این کار را انجام داد. در صورتی که مفعول به یک جاندار اشاره کند سوال با \"چه کسی\" خواهد بود ولی در غیر این صورت چون لیست کلمات جاندار خیلی کامل نیست، سوال با \"چه چیزی/کسی\" خواهد بود.\n",
        "* **تابع esnadi_verbs:** این تابع برای سوال کردن مسند جمله است و با یافتن فعل های اسنادی در جمله، می توان کلمه قبل آن ها را که مسند می باشد را سوال پرسید.\n",
        "* **تابع prepositional_verbs:** این تابع برای سوال کردن متمم های موجود در جمله است و بدین صورت عمل می کند که با یافتن حروف اضافه بر اساس لیست رجکس های مشخص شده، آن ها را سوالی می کند. قابل توجه است که در صورتی که متمم موجود زنده باشد، با \"چه کسی\"، در صورتی که مکان باشد با \"کجا\" و در صورتی که هیچ کدام از آن ها نباشید با \"چه چیزی\" سوال خواهد کرد.\n",
        "* **تابع do_questions:** این تابع برای سوال پرسیدن با \"آیا\" بکار می رود و با تشخیص مثبت یا منفی بودن جمله جواب بله یا خیر می دهد.\n",
        "* **تابع including_keywords:** این تابع برای تولید پرسش و پاسخ در جملاتی به کار می رود که کلید واژه هایی نظیر زیرا، چون، چراکه، باعث، دلیل، عامل و ... را دارند.\n",
        "* **تابع start_with_pronoun:** این تابع برای تولید پرسش و پاسخ در جملاتی به کار می رود که با ضمیر آغاز می شوند و سوالاتی با فرمت چه کسی ....؟ تولید می کند.\n",
        "* **تابع when_questions:** این تابع برای تولید پرسش و پاسخ هایی تولید می شود که در آن ها زمان و تاریخ ذکر شده باشد و سوالاتی با فرمت چه زمانی ... ؟ تولید می کند.\n",
        "* **تابع number_question:** این تابع برای سوال کردن در مورد اعداد در جمله است و در مورد اینکه آن عدد چند است سوال میپرسد.\n",
        "* **تابع adjective_additive:** این تابع برای سوال در مورد صفت در ترکیب وصفی یا مضاف الیه در ترکیب اضافی است. اگر ترکیب اضافی باشد و مضاف الیه یک فرد باشد از \"چه کسی؟\" استفاده میکند در غیر این صورت از \"چه + مضاف + ی\" برای پرسش استفاده میکند.\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "Fs9RSt6-GuWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "VgJY4TG-fLE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "str1 = \"از این موضوع می توان نتیجه گرفت که اتهام مربوطه به او وارد نیست.\"\n",
        "str2 = \"حرکت بار الکتریکی باعث ایجاد میدان الکترومغناطیسی در فضا می شود.\"\n",
        "str3 = \"سرور دانشگاه پایین است چرا که دیروز مشکل سخت افزاری پیش آمد.\"\n",
        "str4 = \"جاذبه باعث افتادن سیب از درخت می شود.\"\n",
        "str5 = \"در امتحان موفق شدم زیرا تمرین های زیادی حل کردم.\"\n",
        "str6 = \"من غذا را می خورم.\"\n",
        "str7 = \"من از قبولی در دانشگاه دلسرد گشتم.\"\n",
        "str8 = \"فردا صبح به شیراز سفر می کنم.\"\n",
        "str9 = \"در امتحان موفق نشدم زیرا تمرین های زیادی حل نکردم.\"\n",
        "str10 = \"من دنبال او گشتم.\"\n",
        "str11 = \"من به دانشگاه رفتم و ناهار را خوردم.\"\n",
        "str12 = \"ما به دانشگاه رفتیم و ناهار را خوردیم.\"\n",
        "str13 = \"دوستم غذایش را خورد.\"\n",
        "str14 = \"من نیوشا را دیدم.\"\n",
        "str15 = \"غذا دیروز من را به بیمارستان کشاند.\"\n",
        "str16 = \"سوختن کامپیوترم عامل قطعی برق دیروز بود.\"\n",
        "str17 = \"من فردا به تبریز سفر خواهم کرد.\"\n",
        "str18 = \"من فردا به تبریز سفر می کنم.\"\n",
        "str19 = \"من از خانه بیرون رفتم و سارا را دیدم.\"\n",
        "str20 = \"من سال ۱۳۷۵ شمسی به دنیا آمده ام.\""
      ],
      "metadata": {
        "id": "KxZimpXzCBuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model  = QAExtraction()\n",
        "input_text = str1\n",
        "result = model.run(input_text)\n",
        "print(json.dumps(result, indent=4, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "763PWfuOTusk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0e8d34-aa46-4f2d-b841-4f5130bd77d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"Question\": \"از این موضوع می توان نتیجه گرفت که اتهام مربوطه به چه کسی وارد نیست؟\",\n",
            "        \"Answer\": \"او\"\n",
            "    },\n",
            "    {\n",
            "        \"Question\": \"آیا از این موضوع می توان نتیجه گرفت که اتهام مربوطه به او وارد نیست؟\",\n",
            "        \"Answer\": \"بله.\"\n",
            "    },\n",
            "    {\n",
            "        \"Question\": \"از این موضوع چه نتیجه ای می توان گرفت؟\",\n",
            "        \"Answer\": \"که اتهام مربوطه به او وارد نیست.\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strs = [eval(\"str\" + str(i)) for i in range(1, 21)]\n",
        "for s in strs:\n",
        "    print(s)\n",
        "    input_text = s\n",
        "    result = model.run(input_text)\n",
        "    print(json.dumps(result, indent=4, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "jB4v9nXULCuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "5OoZ2bBjdpIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Persian names dataset from [here](https://www.kaggle.com/datasets/rezaali/dataset-for-names-of-people-in-persian-language)\n",
        "*   Places dataset from [here](https://github.com/language-ml/parsi.io)\n",
        "*   Verb roots from [here](https://www.peykaregan.ir/dataset/%D9%85%D8%AC%D9%85%D9%88%D8%B9%D9%87-%D8%A7%D9%81%D8%B9%D8%A7%D9%84-%D8%AA%D8%B5%D8%B1%DB%8C%D9%81%E2%80%8C%D8%B4%D8%AF%D9%87-%D9%81%D8%A7%D8%B1%D8%B3%DB%8C)\n",
        "\n"
      ],
      "metadata": {
        "id": "V7SNClQYdsRS"
      }
    }
  ]
}